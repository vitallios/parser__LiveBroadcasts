import requests
import json
import re
from bs4 import BeautifulSoup
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import logging
import sys
import io

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler('parser.log'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG = {
    'base_url': 'https://srrb.ru/category/translyacii-sportivnyx-sobytij',
    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'timeout': 15,
    'max_workers': 4,
    'retry_strategy': {
        'total': 3,
        'backoff_factor': 1,
        'status_forcelist': [500, 502, 503, 504]
    },
    'verify_ssl': False
}

CATEGORIES = {
    "—Ö–æ–∫–∫–µ–π": ["–í–•–õ", "–•–ö", "–ú–•–õ", "–ö–•–õ", "–ñ–•–õ", "—Ö–æ–∫–∫–µ–π"],
    "–±–∞—Å–∫–µ—Ç–±–æ–ª": ["–ë–∞—Å–∫–µ—Ç–±–æ–ª", "–ù–ë–ê", "–ï–≤—Ä–æ–ª–∏–≥–∞"],
    "—Ç–µ–Ω–Ω–∏—Å": ["ATP", "–¢–µ–Ω–Ω–∏—Å", "WTA", "US Open", "–†–æ–ª–∞–Ω –ì–∞—Ä—Ä–æ—Å", "–£–∏–º–±–ª–¥–æ–Ω"],
    "—Ä–µ–≥–±–∏": ["–†–µ–≥–±–∏", "–ü—Ä–æ –î2"],
    "—Ñ—É—Ç–±–æ–ª": ["–§—É—Ç–±–æ–ª", "–õ–∏–≥–∞ —á–µ–º–ø–∏–æ–Ω–æ–≤", "–ü—Ä–µ–º—å–µ—Ä-–ª–∏–≥–∞"],
    "–≤–µ–ª–æ—Å–ø–æ—Ä—Ç": ["–í–µ–ª–æ—Å–ø–æ—Ä—Ç"],
    "–≥–∞–Ω–¥–±–æ–ª": ["–ì–∞–Ω–¥–±–æ–ª"],
    "–±–æ–∫—Å": ["–±–æ–∫—Å—É"],
    "–¥—Ä—É–≥–æ–µ": []
}

class SportStreamParser:
    def __init__(self):
        self.session = self._create_session()
        self.strime_list = []
        self.today = datetime.now().date()
        self.today_str = self.today.strftime("%d.%m.%Y")

    def _create_session(self):
        session = requests.Session()
        retry = Retry(
            total=CONFIG['retry_strategy']['total'],
            backoff_factor=CONFIG['retry_strategy']['backoff_factor'],
            status_forcelist=CONFIG['retry_strategy']['status_forcelist']
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        session.headers.update({
            'User-Agent': CONFIG['user_agent'],
            'Accept-Language': 'ru-RU,ru;q=0.9',
            'Accept-Encoding': 'gzip, deflate'
        })
        return session

    def _get_page(self, url):
        try:
            response = self.session.get(url, timeout=CONFIG['timeout'], verify=CONFIG['verify_ssl'])
            response.encoding = 'utf-8'
            response.raise_for_status()
            return BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {str(e)}")
            return None

    def _get_stream_info(self, url):
        soup = self._get_page(url)
        if not soup:
            return None, None

        # –ü–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
        time_text = None
        content = soup.find('div', class_='entry-content')
        if content:
            for p in content.find_all('p'):
                text = p.get_text().lower()
                if any(kw in text for kw in ['–ø—Ä—è–º–æ–π —ç—Ñ–∏—Ä', '–Ω–∞—á–∞–ª–æ', '—Ç—Ä–∞–Ω—Å–ª—è—Ü–∏—è', '–º—Å–∫']):
                    time_match = re.search(r'(\d{1,2}:\d{2})', text)
                    if time_match:
                        time_text = time_match.group(1)
                        break

        # –ü–æ–∏—Å–∫ iframe
        iframe = soup.find('iframe', {'src': True})
        iframe_html = str(iframe) if iframe else None

        return time_text, iframe_html

    def _process_post(self, post):
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            title_tag = post.find('h3').find('a')
            if not title_tag:
                return None
                
            title = title_tag.get_text().strip()
            link = title_tag['href']
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            date_tag = post.find('span', class_='item-metadata posts-date').find('a')
            if not date_tag:
                return None
                
            try:
                post_date = datetime.strptime(date_tag.get_text().strip(), "%d.%m.%Y").date()
            except ValueError:
                return None
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ —Å–æ–±—ã—Ç–∏–µ —Å–µ–≥–æ–¥–Ω—è
            if post_date != self.today:
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏—è: {title} (–¥–∞—Ç–∞: {post_date})")
                return None
                
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            img_tag = post.find('img')
            img_src = img_tag['src'] if img_tag else None
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category_tag = post.find('li', class_='meta-category').find('a')
            category_text = category_tag.get_text().strip() if category_tag else ""
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            time_match = re.search(r'–≤ (\d{1,2}:\d{2})', title)
            time_str = time_match.group(1) if time_match else None
            
            # –ï—Å–ª–∏ –≤—Ä–µ–º—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ, –∏—â–µ–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            if not time_str:
                time_str, iframe = self._get_stream_info(link)
            else:
                _, iframe = self._get_stream_info(link)
                
            if not iframe:
                return None
                
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category = "–¥—Ä—É–≥–æ–µ"
            for cat, keywords in CATEGORIES.items():
                if any(kw.lower() in category_text.lower() or kw.lower() in title.lower() for kw in keywords):
                    category = cat
                    break
                    
            # –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
            clean_title = re.sub(r'\. –ü—Ä—è–º–∞—è —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏—è \d{2}\.\d{2}\.\d{4} –≤ \d{1,2}:\d{2}', '', title)
            clean_title = clean_title.replace('–°–º–æ—Ç—Ä–µ—Ç—å –æ–Ω–ª–∞–π–Ω:', '').strip()
            
            return {
                "category": category,
                "name": clean_title,
                "link": iframe,
                "data": post_date.strftime("%Y.%m.%d"),
                "time": time_str,
                "img": img_src, 
                "premium": "",
                "active": 0
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–∞: {str(e)}", exc_info=True)
            return None

    def _generate_telegram_post(self):
        if not self.strime_list:
            return "üèüÔ∏è –ù–∞ —Å–µ–≥–æ–¥–Ω—è —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ üèüÔ∏è"

        categorized = {}
        for item in self.strime_list:
            if item['category'] not in categorized:
                categorized[item['category']] = []
            categorized[item['category']].append(item)

        category_emojis = {
            "—Ñ—É—Ç–±–æ–ª": "‚öΩÔ∏è",
            "—Ç–µ–Ω–Ω–∏—Å": "üéæ",
            "—Ö–æ–∫–∫–µ–π": "üèí",
            "–±–∞—Å–∫–µ—Ç–±–æ–ª": "üèÄ",
            "–≤–µ–ª–æ—Å–ø–æ—Ä—Ç": "üö¥",
            "–≥–∞–Ω–¥–±–æ–ª": "ü§æ",
            "—Ä–µ–≥–±–∏": "üèâ",
            "–±–æ–∫—Å": "ü•ä",
            "–¥—Ä—É–≥–æ–µ": "üèüÔ∏è"
        }

        post_lines = [
            f"üèüÔ∏è –°–ø–æ—Ä—Ç–∏–≤–Ω—ã–µ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ –Ω–∞ {self.today_str} üèüÔ∏è",
            "",
            "üìÖ –°–µ–≥–æ–¥–Ω—è –≤ —ç—Ñ–∏—Ä–µ:",
            ""
        ]

        preferred_order = ["—Ñ—É—Ç–±–æ–ª", "—Ç–µ–Ω–Ω–∏—Å", "—Ö–æ–∫–∫–µ–π", "–±–∞—Å–∫–µ—Ç–±–æ–ª", "–≤–µ–ª–æ—Å–ø–æ—Ä—Ç", "—Ä–µ–≥–±–∏", "–≥–∞–Ω–¥–±–æ–ª", "–±–æ–∫—Å"]
        sorted_categories = sorted(
            categorized.keys(),
            key=lambda x: preferred_order.index(x) if x in preferred_order else len(preferred_order))
        
        for category in sorted_categories:
            emoji = category_emojis.get(category, "üèüÔ∏è")
            post_lines.append(f"{emoji} {category.capitalize()}")
            
            for item in categorized[category]:
                post_lines.append(f"‚è∞ {item['time']} - {item['name']}")
            post_lines.append("")

        post_lines.extend([
            "üì∫ @Live_Strim_bot",
            "",
            "üìå –ù–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –º–∞—Ç—á–∏!",
            "#—Å–ø–æ—Ä—Ç #—Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ #—Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–π–∫–∞–ª–µ–Ω–¥–∞—Ä—å"
        ])

        return "\n".join(post_lines)

    def parse(self):
        logger.info(f"–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π –Ω–∞ {self.today_str}")
        
        try:
            soup = self._get_page(CONFIG['base_url'])
            if not soup:
                raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")

            posts_container = soup.find('div', id='aft-archive-wrapper')
            if not posts_container:
                raise Exception("–ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –ø–æ—Å—Ç–∞–º–∏")

            posts = posts_container.find_all('article', class_='af-sec-post')
            if not posts:
                logger.warning("–ü–æ—Å—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

            with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:
                results = list(executor.map(self._process_post, posts))
                self.strime_list = [r for r in results if r]

            if self.strime_list:
                with open('strimeList.json', 'w', encoding='utf-8') as f:
                    json.dump(self.strime_list, f, ensure_ascii=False, indent=2)
                logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.strime_list)} —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π –Ω–∞ —Å–µ–≥–æ–¥–Ω—è")

                telegram_post = self._generate_telegram_post()
                with open('telegram_post.txt', 'w', encoding='utf-8') as f:
                    f.write(telegram_post)
                logger.info("–ü–æ—Å—Ç –¥–ª—è Telegram —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
                
                print("\n" + "="*50)
                print(telegram_post)
                print("="*50 + "\n")
            else:
                logger.info("–ù–µ—Ç —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π –Ω–∞ —Å–µ–≥–æ–¥–Ω—è")

        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}", exc_info=True)

if __name__ == '__main__':
    parser = SportStreamParser()
    parser.parse()
