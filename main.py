import requests
import json
import re
from bs4 import BeautifulSoup
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler('parser.log'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG = {
    'base_url': 'https://srrb.ru/category/translyacii-sportivnyx-sobytij',
    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'timeout': 15,
    'max_workers': 4,
    'retry_strategy': {
        'total': 3,
        'backoff_factor': 1,
        'status_forcelist': [500, 502, 503, 504]
    }
}

CATEGORIES = {
    "—Ö–æ–∫–∫–µ–π": ["–í–•–õ", "–•–ö", "–ú–•–õ", "–ö–•–õ", "–ñ–•–õ", "—Ö–æ–∫–∫–µ–π"],
    "–±–∞—Å–∫–µ—Ç–±–æ–ª": ["–ë–∞—Å–∫–µ—Ç–±–æ–ª", "–ù–ë–ê", "–ï–≤—Ä–æ–ª–∏–≥–∞"],
    "—Ç–µ–Ω–Ω–∏—Å": ["ATP", "–¢–µ–Ω–Ω–∏—Å", "WTA", "US Open", "–†–æ–ª–∞–Ω –ì–∞—Ä—Ä–æ—Å"],
    "—Ä–µ–≥–±–∏": ["–†–µ–≥–±–∏", "–ü—Ä–æ –î2"],
    "—Ñ—É—Ç–±–æ–ª": ["–§—É—Ç–±–æ–ª", "–õ–∏–≥–∞ —á–µ–º–ø–∏–æ–Ω–æ–≤", "–ü—Ä–µ–º—å–µ—Ä-–ª–∏–≥–∞"],
    "–≤–µ–ª–æ—Å–ø–æ—Ä—Ç": ["–í–µ–ª–æ—Å–ø–æ—Ä—Ç"],
    "–≥–∞–Ω–¥–±–æ–ª": ["–ì–∞–Ω–¥–±–æ–ª"],
    "–±–æ–∫—Å": ["–±–æ–∫—Å—É"],
    "–¥—Ä—É–≥–æ–µ": []
}

class SportStreamParser:
    def __init__(self):
        self.session = self._create_session()
        self.strime_list = []
        self.today = datetime.now().date()
        self.today_str = self.today.strftime("%d.%m.%Y")

    def _create_session(self):
        session = requests.Session()
        retry = Retry(
            total=CONFIG['retry_strategy']['total'],
            backoff_factor=CONFIG['retry_strategy']['backoff_factor'],
            status_forcelist=CONFIG['retry_strategy']['status_forcelist']
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        session.headers.update({
            'User-Agent': CONFIG['user_agent'],
            'Accept-Language': 'ru-RU,ru;q=0.9',
            'Accept-Encoding': 'gzip, deflate'
        })
        return session

    def _extract_date(self, title):
        month_map = {
            '—è–Ω–≤–∞—Ä—è': '01', '—Ñ–µ–≤—Ä–∞–ª—è': '02', '–º–∞—Ä—Ç–∞': '03',
            '–∞–ø—Ä–µ–ª—è': '04', '–º–∞—è': '05', '–∏—é–Ω—è': '06',
            '–∏—é–ª—è': '07', '–∞–≤–≥—É—Å—Ç–∞': '08', '—Å–µ–Ω—Ç—è–±—Ä—è': '09',
            '–æ–∫—Ç—è–±—Ä—è': '10', '–Ω–æ—è–±—Ä—è': '11', '–¥–µ–∫–∞–±—Ä—è': '12'
        }

        dd_mm_yyyy = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', title)
        if dd_mm_yyyy:
            day, month, year = dd_mm_yyyy.groups()
            return datetime(int(year), int(month), int(day)).date()

        for month_ru, month_num in month_map.items():
            pattern = rf'(\d{{1,2}})\s+{month_ru}\s+(\d{{4}})'
            match = re.search(pattern, title)
            if match:
                day, year = match.groups()
                return datetime(int(year), int(month_num), int(day)).date()

        return None

    def _get_page(self, url):
        try:
            response = self.session.get(url, timeout=CONFIG['timeout'])
            response.encoding = 'utf-8'
            response.raise_for_status()
            return BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {str(e)}")
            return None

    def _get_stream_info(self, url):
        soup = self._get_page(url)
        if not soup:
            return None, None

        time_text = None
        content = soup.find('div', class_='entry-content')
        if content:
            for p in content.find_all('p'):
                text = p.get_text().lower()
                if any(kw in text for kw in ['–ø—Ä—è–º–æ–π —ç—Ñ–∏—Ä', '–Ω–∞—á–∞–ª–æ', '—Ç—Ä–∞–Ω—Å–ª—è—Ü–∏—è', '–º—Å–∫']):
                    time_text = p.get_text().strip()
                    break

        iframe = soup.find('iframe', {'src': True})
        iframe_html = str(iframe) if iframe else None

        return time_text, iframe_html

    def _clean_title(self, title, date_str, time_str):
        words_to_remove = [
            '–°–º–æ—Ç—Ä–µ—Ç—å', '–æ–Ω–ª–∞–π–Ω', '—Ç—Ä–∞–Ω—Å–ª—è—Ü–∏—è', '—ç—Ñ–∏—Ä',
            date_str.strftime("%d.%m.%Y"), time_str, '–≤', '–º—Å–∫', '‚Äî'
        ]
        words = title.split()
        return ' '.join(w for w in words if w not in words_to_remove)

    def _process_post(self, post):
        try:
            a_tag = post.find('a')
            if not a_tag:
                return None

            link = a_tag.get('href')
            if not link:
                return None

            title = a_tag.get_text().strip()
            event_date = self._extract_date(title)
            
            if not event_date or event_date != self.today:
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏—è: {title} (–¥–∞—Ç–∞: {event_date})")
                return None

            time_text, iframe = self._get_stream_info(link)
            if not time_text or not iframe:
                return None

            time_match = re.search(r'(\d{1,2}:\d{2})', time_text)
            if not time_match:
                return None
            time_str = time_match.group(1)

            clean_title = self._clean_title(title, event_date, time_str)
            if not clean_title:
                return None

            category = "–¥—Ä—É–≥–æ–µ"
            for cat, keywords in CATEGORIES.items():
                if any(kw.lower() in clean_title.lower() for kw in keywords):
                    category = cat
                    break

            img = post.find('img', {'src': True})
            img_src = img['src'] if img else None

            return {
                "category": category,
                "name": clean_title,
                "link": iframe,
                "data": event_date.strftime("%Y.%m.%d"),
                "time": time_str,
                "img": img_src,
                "premium": "",
                "active": 0
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–∞: {str(e)}")
            return None

    def _generate_telegram_post(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –¥–ª—è Telegram –≤ –∑–∞–¥–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
        if not self.strime_list:
            return "üèüÔ∏è –ù–∞ —Å–µ–≥–æ–¥–Ω—è —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ üèüÔ∏è"

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categorized = {}
        for item in self.strime_list:
            if item['category'] not in categorized:
                categorized[item['category']] = []
            categorized[item['category']].append(item)

        # –≠–º–æ–¥–∑–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        category_emojis = {
            "—Ñ—É—Ç–±–æ–ª": "‚öΩÔ∏è",
            "—Ç–µ–Ω–Ω–∏—Å": "üéæ",
            "—Ö–æ–∫–∫–µ–π": "üèí",
            "–±–∞—Å–∫–µ—Ç–±–æ–ª": "üèÄ",
            "–≤–µ–ª–æ—Å–ø–æ—Ä—Ç": "üö¥",
            "–≥–æ–ª—å—Ñ": "üèì",
            "—Ä–µ–≥–±–∏": "üèâ",
            "–≥–∞–Ω–¥–±–æ–ª": "ü§æ",
            "–±–æ–∫—Å": "ü•ä",
            "–¥—Ä—É–≥–æ–µ": "üèüÔ∏è"
        }

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ—Å—Ç
        post_lines = [
            f"üèüÔ∏è –°–ø–æ—Ä—Ç–∏–≤–Ω—ã–µ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ –Ω–∞ {self.today_str} üèüÔ∏è",
            "",
            "üìÖ –°–µ–≥–æ–¥–Ω—è –≤ —ç—Ñ–∏—Ä–µ:",
            ""
        ]

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É –∏–∑ –ø—Ä–∏–º–µ—Ä–∞
        preferred_order = ["—Ñ—É—Ç–±–æ–ª", "—Ç–µ–Ω–Ω–∏—Å", "—Ö–æ–∫–∫–µ–π", "–±–∞—Å–∫–µ—Ç–±–æ–ª", "–≤–µ–ª–æ—Å–ø–æ—Ä—Ç", "–≥–æ–ª—å—Ñ", "—Ä–µ–≥–±–∏", "–≥–∞–Ω–¥–±–æ–ª", "–±–æ–∫—Å"]
        sorted_categories = sorted(
            categorized.keys(),
            key=lambda x: preferred_order.index(x) if x in preferred_order else len(preferred_order))
        
        for category in sorted_categories:
            emoji = category_emojis.get(category, "üèüÔ∏è")
            post_lines.append(f"{emoji} {category.capitalize()}")
            
            for item in categorized[category]:
                post_lines.append(f"‚è∞ {item['time']} - {item['name']}")
            post_lines.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        post_lines.extend([
            "üì∫ @Live_Strim_bot",
            "",
            "üìå –ù–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –º–∞—Ç—á–∏!",
            "#—Å–ø–æ—Ä—Ç #—Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ #—Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–π–∫–∞–ª–µ–Ω–¥–∞—Ä—å"
        ])

        return "\n".join(post_lines)

    def parse(self):
        logger.info(f"–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π –Ω–∞ {self.today_str}")
        
        try:
            soup = self._get_page(CONFIG['base_url'])
            if not soup:
                raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")

            posts = soup.find_all('article', class_='post')
            if not posts:
                logger.warning("–ü–æ—Å—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

            with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:
                results = list(executor.map(self._process_post, posts))
                self.strime_list = [r for r in results if r]

            if self.strime_list:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Å –¥–∞–Ω–Ω—ã–º–∏
                with open('strimeList.json', 'w', encoding='utf-8') as f:
                    json.dump(self.strime_list, f, ensure_ascii=False, indent=2)
                logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.strime_list)} —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π –Ω–∞ —Å–µ–≥–æ–¥–Ω—è")

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å—Ç –¥–ª—è Telegram
                telegram_post = self._generate_telegram_post()
                with open('telegram_post.txt', 'w', encoding='utf-8') as f:
                    f.write(telegram_post)
                logger.info("–ü–æ—Å—Ç –¥–ª—è Telegram —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
                
                # –í—ã–≤–æ–¥–∏–º –ø–æ—Å—Ç –≤ –∫–æ–Ω—Å–æ–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                print("\n" + "="*50)
                print(telegram_post)
                print("="*50 + "\n")
            else:
                logger.info("–ù–µ—Ç —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π –Ω–∞ —Å–µ–≥–æ–¥–Ω—è")

        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")

if __name__ == '__main__':
    parser = SportStreamParser()
    parser.parse()
